local {
  ollama {
    host = "http://localhost:11434"
    model = "llama3.2"
    request-timeout-seconds = 500
    range = 2
  }
  server="localhost"
  conversationPath = "src/main/resources/conversation-agents"
}

docker {
  ollama {
    host = "http://ollama-container:11434"
    model = "llama3.2"
    request-timeout-seconds = 500
    range = 1
  }
  server="0.0.0.0"
  conversationPath = "/llm/conversation-agents"
}

cloud {
  ollama {
    host = "http://localhost:11434"
    model = "llama3.2"
    request-timeout-seconds = 20
    range = 2
  }
  server="localhost"
  conversationPath = "s3://nithish-llm-hw3/output/conversation-agents"
  region = "us-east-2"
  access-key = "AKIA5G2VGFIU47D3LYMU"
  secret-key = "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"
}


lambdaApiGateway = "https://tfz33jek7j.execute-api.us-east-2.amazonaws.com/PRODStage/queryLLM"

maxWords = 100
