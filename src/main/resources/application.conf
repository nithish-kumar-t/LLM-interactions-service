lambdaApiGateway = "https://tfz33jek7j.execute-api.us-east-2.amazonaws.com/PRODStage/queryLLM"

maxWords = 100


local {
  ollama {
    host = "http://localhost:11434"
    model = "llama3.2"
    request-timeout-seconds = 500
    range = 2
  }
  server="localhost"
  conversationPath = "src/main/resources/conversation-agents"
}

docker {
  ollama {
    host = "http://ollama-container:11434"
    model = "llama3.2"
    request-timeout-seconds = 500
    range = 1
  }
  conversationPath = "/llm/conversation-agents"
  server="0.0.0.0"
}
